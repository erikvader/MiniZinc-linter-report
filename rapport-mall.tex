\documentclass[a4paper,12pt]{article}
\usepackage{silence}
\WarningFilter{latex}{Command \underline  has changed} % don't know where these come from
\WarningFilter{latex}{Command \underbar  has changed}
\usepackage[dvipsnames]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[parfill]{parskip}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{csvsimple}
% \usepackage[perpage]{footmisc}
\usepackage{tabularx}
\usepackage[nottoc,numbib]{tocbibind}

\newif\iffast
% \fasttrue
\input{common.tex}

\usepackage{UppsalaExjobb}

\newcommand{\leblanc}{\clearpage\thispagestyle{empty}\null\clearpage}
\newcommand{\ruleref}[1]{``\nameref{sec:rule:#1}'' (Section~\ref{sec:rule:#1})}

\begin{document}
% För att ställa in parametrar till IEEEtranS/IEEEtranSA behöver detta ligga här (före första \cite).
% Se se IEEEtran/bibtex/IEEEtran_bst_HOWTO.pdf, avsnitt VII, eller sista biten av IEEEtran/bibtex/IEEEexample.bib.
%%%% OBS: här ställer ni t.ex. in hur URLer ska beskrivas.
\bstctlcite{rapport:BSTcontrol}

\title{Implementing a Linter for Static Analysis of MiniZinc Models}
%\subtitle{beskrivande men gärna lockande}

\author{Erik Rimskog}

% Visa datum på svenska på förstasidan, även om ni skriver på engelska!
\date{\begin{otherlanguage}{swedish}
\today
\end{otherlanguage}}

\handledare{Pierre Flener}
\reviewer{Justin Pearson}
\examinator{Lars-Åke Nordén}
\seriesname{Examensarbete 30 hp}

\maketitle

\leblanc

% Change to frontmatter style (e.g. roman page numbers)
\frontmatter

\begin{abstract}
% \input{instr-abstract}
MiniZinc is a modelling language for constraint satisfaction and optimisation problems.
It can be used to solve difficult problems by declaratively modelling them and giving
them to a generic solver.
A linter, a tool for static analysis, is implemented for MiniZinc to provide analysis
for improving models.
Suggesting rewrites that will speed up solving, removing unnecessary constructs and
pointing out potential problems are examples of analysis this tool provides.
A method for finding points of interest in Abstract Syntax Trees (parsed models) is designed and implemented.
The linter is tested and evaluated against models in the MiniZinc Benchmarks,
a collection of models used to benchmark solvers.
The results from running the linter on one of the models from the benchmarks is more
closely inspected and evaluated.
The suggestions were correct and made the model simpler.
Performance measurements showed that most suggestions worsened, or did not affect, the performance,
depending on the solver.
\end{abstract}

\leblanc

\begin{sammanfattning}
Statisk kodanalys är en metod för att fånga många olika sorters fel och buggar innan
ett program körs. En \emph{linter} är ett samlingsnamn på verktyg som utför
statisk analys, och denna rapport beskriver hur en linter för modelleringsspråket MiniZinc
skapades.

Det finns många problem som är väldigt svåra att lösa då det inte finns några algoritmer
som kan lösa dem på polynomisk tid. Med andra ord finns det inga kända algoritmer som kan lösa
dem problemen ``snabbt'', utan en approximering eller fullständig sökning måste genomföras.
Oftast är en fullständig sökning för långsam, det kan t.o.m.\@ i vissa fall ta flera miljoner år att gå
igenom alla potentiella lösningar, även för relativt små problem.
Ett exempel på typiska svåra problem är schemaläggning och att hitta den kortaste rutten
mellan t.ex.\@ städer.

Det finns generalla program som kallas för \emph{lösare} som löser ett problem med hjälp
utav specialiserade algoritmer och metoder som kan hitta en lösning mycket snabbare än att
naivt söka igenom alla lösningar. Lösarna tar oftast en deklarativ beskrivning
av problemet de ska lösa, kallad modell. Olika lösare är bra på olika sorters problem, så
flera lösare behöver testas för ett visst problem för att se vilken som är bäst.

MiniZinc är ett verktyg för att modellera optimeringsproblem på ett ``universalt'' vis, en
och samma modell kan användas på flera olika lösare utan att behöva skriva om modellen för
varje. MiniZinc modellerar problem med hjälp utav beslutsvariabler (decision
variable) och begränsningar (constraint) på dessa variabler. En lösares uppgift är då att
hitta värden på dessa variabler så att alla begränsingar uppfylls.

Det är lätt att skriva modeller som inte beskriver ett problem i fråga på bästa möjliga
sätt. Konsekvensen blir oftast att det tar längre tid för lösaren att hitta en lösning.
Lintern beskriven i denna rapport letar främst efter vanliga fel som allmänt är kända att
göra det svårare för en lösare att hitta en lösning. Potentiella fel hittas med hjälp utav
en egengjord sökare av abstrakta syntaxträd, vilket är ett formellt sätt att representera källkod på.

Lintern evaluerades på en utvald modell från ``MiniZinc Benchmarks'', en samling av
modeller för prestandamätning. Modellen blev enklare och mer tydlig, men prestandan blev
förvånandsvärt nog generellt sämre, i alla fall på en lösare. På en annan lösare, som
använder sig utav helt andra algoritmer, blev det ingen märkbar skillnad på lösningstiden.
\end{sammanfattning}

\leblanc

\tableofcontents

\cleardoublepage

% Change to main matter style (arabic page numbers, reset page numbers)
\mainmatter

\section{Introduction}\label{sec:introduktion}
Static analysis on source code is a way to catch many kinds of errors before
a program even is executed.
Bugs, weird special cases, and inefficient code are some of many things that are possible
to statically check, and tools that do this are called \emph{linters}.

MiniZinc is a declarative solver-independent modelling language for constraint
satisfaction and optimisation problems~\cite{MiniZinc}. The MiniZinc compiler reports on
syntax errors and other errors that inhibit a successful compilation. It does not suggest
improvements on the model itself by, e.g.\@, reporting on constructs (Syntactically valid
sequence of tokens, e.g.\@, if-statements) that should be avoided if possible, or on
constructs that are error prone. This is not surprising since a compiler's main job is to
process code into some other code (e.g.\@ machine code). If the construct is valid, then a compiler will
process it, else abort. The goal of this project is to implement a linter that do these model
improving checks for MiniZinc models. The exact checks (or rules) implemented are all
described in Section~\ref{sec:rules}.

Constraint satisfaction and optimisation problems have many important applications, such
as scheduling and finding optimal routes. Problems like these can be difficult to solve as
they often do not have a known polynomial-time algorithm. There are programs called
\emph{solvers} that are made to find solutions to problems like this using advanced
algorithms in different technologies, each one good in its own way. Problems are usually
described declaratively as models, where each solver has its own way of specifying them.
MiniZinc is an effort to create a modelling language that can be used with many different
solvers, making it a lot easier to try different solvers with the same problem
description. The core of constraint problems consist of \emph{constraints} and
\emph{decision variables}. A decision variable is a variable with unknown value, and a
solver's task is to find appropriate values for all of these. Constraints specify how
decision variables relate to each other, or what values are allowed, e.g., that one
variable must be greater than another.

Declarative models specify \emph{what} problem to solve, not the exact steps to \emph{how}
it should be solved. It is therefore easy to write inefficient models without knowing it,
since, a small edit can greatly affect solving time. A tool to point out potential issues
is attractive to have, especially for beginners as they might not know what implications
each aspect of a model have and its impact on solving time. For example, the domain of
decision variables should be as small as possible to limit the search space, but if a
domain is not specified, then the domain can potentially be as big as the solver can
handle, which can slow down solving.

To save time, simplifying the implementation and to allow for a possible integration with
the MiniZinc project, was the parser from the MiniZinc project reused for this project,
specifically the output from it, namely an Abstract Syntax Tree (AST). An AST is a data
structure encoding the formal structure of a program, a MiniZinc model in this case. The
MiniZinc compiler is written in the programming language C++~\cite{cpp}, so this project
is also written in C++ to be able to use the same parser as easily as possible.
Searching
for relevant constructs in the MiniZinc AST is one of the main tasks of this project. More
details on how the AST is searched is described in Section~\ref{sec:impl}.

\begin{sloppypar}
The linter was run on all models in the MiniZinc Benchmarks~\cite{mznbench}, a
big repository of models and instances where most come from earlier MiniZinc
challenges~\cite{MZN:Challenge}, a competition for solvers. The results from that is
discussed in Section~\ref{sec:discussion}. An in-depth look into one of the models in
these benchmarks is explored in Section~\ref{sec:nsp}, the suggestions from the linter are
performed and benchmarked.
\end{sloppypar}

\section{Background}\label{sec:bakgrund}
This section will give necessary background information about: MiniZinc, what kinds of
problems it can solve, how a MiniZinc model is written, more on linters, and what
an abstract syntax tree is.

\subsection{MiniZinc}\label{sec:mzn}
Combinatorial problems are problems of discrete states where some of these states are
solution states~\cite{combopt}. Combinatorial satisfaction tries to find these solution states,
while combinatorial optimisation tries to find a solution state that minimises or
maximises some quantity associated with each state.

MiniZinc models combinatorial problems using constraints, i.e.\@, constraint satisfaction
and optimisation problems. These are modelled using decision variables and
constraints on those variables~\cite{constraintshandbook}. Each constraint limits the
allowed values on one or more variables.
For example $x_1 > x_2$ or $x_3 = 3$ etc.
A solution state from all current variable assignments is formed when all constraints are
satisfied.

There are many solvers and technologies for solving combinatorial optimisation and
satisfaction problems, each one good in its own way. Each solver
has its own interface, which makes it time consuming to compare different
solvers as the same problem has to be reformulated for each one. MiniZinc~\cite{MiniZinc}
is a tool chain for creating ``universal'' models that can be run on any solver MiniZinc
supports, i.e., the \emph{same} model can be used on different solvers without any
rewrites.

A diagram showing how a MiniZinc model is processed to eventually run on a
solver is shown in Figure~\ref{fig:minizinc}. The model itself is written in a text file
that is compiled (or flattened) into something called FlatZinc, a lower level
representation of the model designed to be easily used by solvers. Each
solver has its own backend, which is the program or library that actually converts the
FlatZinc model into the solver's own model that then can be used by the solver.

\begin{figure}[ht]
  \centering
  \input{fig-minizinc}
  \caption{How a MiniZinc model gets run by a solver to produce a result. The high-level
    model is compiled (flattened) into a low-level model that a backend and solver pair
    uses to find solutions.}%
  \label{fig:minizinc}
\end{figure}

MiniZinc models are often divided into two parts: the model itself and instances. The
model is made generic in the sense that it models some problem, without exact values. For
example, some model could be modelled in terms of a parameter $n$, but to be solved by a
solver must $n$ have a concrete value. Instances, usually separate files, provides
concrete values to these parameters. This distinction makes it easy to run the same model
with slightly different values on all parameters.

\subsubsection{Syntax Overview}\label{sec:mzn:syntax}
A MiniZinc model consists primarily of decision variables, constraints,
functions, and solve statements. Decision variables are values of various types like
\mi{int}, \mi{bool}, and \mi{float}, which the solver should find appropriate values for.
What values are allowed and how the variables should relate to each other is expressed in
constraints. For example, the following defines a decision variable \mi{x} of type
\mi{int}, and a constraint that constrains \mi{x} to have a value strictly greater than
five.
\begin{mznnobreak}[label=lst:overview1,caption={\null}]
var int: x;
constraint x > 5;
\end{mznnobreak}

\begin{sloppypar}
There are two solving modes, specified in a \mi{solve}-statement: satisfaction and optimisation. Satisfaction
finds values for all decision variables such that all constraints are satisfied, and
optimisation additionally minimises or maximises some decision variable.
For example, adding \mi{solve minimize x} to the example above (Listing~\ref{lst:overview1}) would create a
model that tries to find the smallest integer value greater than five, which is
six. Adding \mi{solve satisfy} would instead find \emph{any} integer greater than
five. An optional search annotation can be specified on a \mi{solve}-statement, specifying
a hint that some solvers take into account. The following is an annotation that
hints how \mi{x} should be searched:
\end{sloppypar}
\begin{mznnobreak}
:: int_search([x], input_order, indomain_min)
\end{mznnobreak}
The first argument is an array of variables to specify a hint for, the
second argument gives the order in which the variables in the array should be searched in,
and the last argument specify what value the variables should be assigned given their
current computed domains. In this example, \mi{indomain_min}, specifies that the minimum
value in the current domain should be tried first.

Variables that are fixed to a single given value are in MiniZinc called \emph{parameters}.
A parameter is declared with \mi{par}, or no keyword at all, instead of \mi{var}.
Parameters are useful since they can be seen as parameters for the whole model itself,
i.e., what exact instance of a model that should be solved.
For example, the above example (Listing~\ref{lst:overview1}) can be rewritten with a parameter as follows:
\begin{mznnobreak}
int: n = 5;
var int: x;
constraint x > n;
solve minimize x;
\end{mznnobreak}
Now the value of \mi{n} can easily be changed to solve a slightly different problem.

This example can be separated into an instance file and a model file by giving
the model the declaration {\mi{int: n}} and the
instance file the assignment \mi{n = 5}. Multiple instance files can be created
for a single model, which makes it possible to change all parameters at once.

Arrays allow for a variable amount of variables. The example below (Listing~\ref{lst:array}) shows an array \mi{xs}
consisting of five int-variables. A variable in the array can be accessed separately with
some special syntax. For example, the first variable can be accessed with
\mi{xs[1]}.
To access all variables in an array at once, can the builtin function \mi{forall} be used.
The example below (Listing~\ref{lst:array}) uses this to constrain all variables in the array to be strictly greater
than five, it is like writing: $\forall x \in xs : x > 5$.
\begin{mznnobreak}[label=lst:array,caption={\null}]
array[1..5] of var int: xs;
constraint forall(i in 1..5)(xs[i] > 5);
\end{mznnobreak}

The final feature that will be covered here is functions. Functions allow the modeller to
reuse the same code in multiple places. A function has a return value and zero or more
parameters (not instance parameters), all of which can have the same possible types as
regular variables (\mi{int} etc.). The following example is a declaration and usage of a function that takes a decision variable as
a parameter and returns a decision variable back:
\begin{mznnobreak}
function var int: a_name(var int: x) = <@\textit{<some expression>}@>;
constraint a_name(x) < 69;
\end{mznnobreak}

Some functions called \emph{global constraints} are special. A global constraint is a
function defined in the standard library for high-level modelling abstractions. There is
one global constraint called \mi{all_different} that constrains all values in an array to
all be different from each other. Many solvers implement special inference algorithms for
global constraints, which basically means that they can reason a lot better about them.
Using a global constraint is preferred over directly using the definition of one.

\subsubsection{Travelling Salesman Problem}\label{sec:tsp}

One example of combinatorial optimisation is the Travelling Salesman Problem (TSP) where a
salesman wants to travel to $n \in \elnaturale^+$ cities in an order that minimises the total
distance travelled. Here, the solution states are all orderings that visits every city once, and the
quantity to minimise is the total distance.

The following is an example of how TSP can be modelled in MiniZinc,
The same instance is illustrated in Figure~\ref{fig:tsp}:
\begin{mznbreak}
include "globals.mzn";
int: N = 4;
set of int: Ns = 1..N;
array[Ns,Ns] of int: dist =
  [| 0,15,20,26
   |15, 0, 5,12
   |20, 5, 0, 5
   |26,12, 5, 0|];
array[Ns] of var Ns: next;
constraint circuit(next);
solve minimize sum(i in Ns)(dist[i,next[i]]);
\end{mznbreak}

\begin{figure}[ht]
  \centering
  \input{fig-tsp}
  \caption{Example instance of Travelling Salesman Problem with $n=4$ cities. The state is
    $\langle 3,4,2,1 \rangle$, which means that city 3 is visited after city 1, city 4 is
    visited after city 2, etc. The total distance travelled is \SI{63}{km}. Only the paths
    for the current route are shown.}%
  \label{fig:tsp}
\end{figure}

TSP could be modelled as a list of length $N$ with integers from 1 to $N$ where each
integer represent a city (line~2 and line~3). A 2D-array, or matrix, is constructed on
line~4 with the distances between all pair of cities. \mi{dist[i,j]} is the distance
between city \mi{i} and \mi{j}, which is the same as the distance \mi{dist[j,i]}. The
actual route to travel is encoded as an array on line~9, that is the decision variable(s).
The value \mi{next[i]} is what city to visit next, e.g., \mi{next[1] = 2} means that
city~2 shall be travelled to after city~1. The global constraint \mi{circuit} on line~10
is constraining \mi{next} to complete a circuit (path) between all cities. Example of a
circuit is $\langle 2,3,4,1 \rangle$, and an invalid circuit is $\langle 1,1,2,3 \rangle$.
The global constraint must be imported from the standard library on line~1. Finally,
line~11 specify the quantity to minimise, which is the total distance in this case. The
distance of a city to itself can be anything as \mi{circuit} prohibit a city to have
itself as next destination.

\subsection{Linters}\label{sec:bkg:linter}
% Lint is a program for doing static code analysis of C programs~\cite{lint}. The term
% \emph{linter} originates from this program and is nowadays a general term for any tool
% that performs static code analysis, except for things a compiler typically does, like
% syntax analysis and semantic analysis (type checking). Static in this context means that
% the analysis is done on the source code itself, without executing it.

A linter is a program for doing static analysis of source code, \emph{static} meaning that
the analysis is done without running the program. The term \emph{linter} originates from a
program called lint~\cite{lint}, a tool for analysing C~\cite{c1978} programs. A linter can look for
bugs, enforce a certain code style, find some kind of improvement, find error prone
constructs, point out special cases etc. Compilers also do static analysis when they are
compiling, e.g., syntax analysis (whether a sequence of characters encode a valid
construct) and semantic analysis (type checking etc.\@).

A compiler's main job is to convert source code to some other source code, usually machine
code, and is probably aiming at doing so as fast as possible. A compiler generally only
does the minimum amount of analysis required to determine if the given source code is
legal, i.e., follow the language's standard. Even though a program is legal does not mean
it is good, it could crash immediately on start up. Linters take a more leisurely view and
tries see whether there are any potential problems with the otherwise legal program. The
linter could look for uninitialised pointers in C programs that are dereferenced (would
most likely make the program crash) and warn about those. Finding uses of variables that
have not been set to a value is something the original lint does~\cite{lint}. Another
distinction is that linters are not as precise as compilers. Linters provide
\emph{suggestions}, it is fine if they are wrong, the worst thing that can happen is that
the user gets annoyed by false positives. A compiler, in contrast, can never be wrong, it
should always be certain about its conclusions, it might not produce correct programs
otherwise.

There also are linters and other static analysis tools for
programming languages that are
interpreted, i.e., languages that are run ``as is'' without compiling them first. Python
is an example of a dynamically typed (types determined at runtime) and interpreted
language. It is possible to put optional type information in a python program and have a
third-party program like mypy~\cite{mypy} do static type checking. Mypy does not
consider itself a linter as it performs type checking, something a compiler typically
do.

The checks a linter does are typically called \emph{rules}, and different linters have
different ways of specifying them. The JavaScript linter ESLint~\cite{ESLint} allows the users
to create their own rules in separate JavaScript files, alongside the builtin ones.
Since JavaScript is interpreted, can new user-specified rules be added relatively easily,
they are loaded as any other file.
A rule in ESLint
consists of a pattern match against the AST and a function
that will do additional checks to see whether it is a match or not and if something
should be reported to the user. The rules in this project are structured in the same
manner, except that the part that pattern matches is not as powerful. The one in ESLint can
find arbitrary sub-trees while the one in this project can only find paths (sub-trees that
are a line) in the AST.

Clippy~\cite{Clippy} is a linter for the programming language Rust~\cite{rustlang}. Rust
is a compiled language and rules to Clippy are specified as Rust source code. This means
that any new custom lint rule has to recompile the whole linter to be used, similar to
this project. Clippy rules are specified in a similar way to ESLint, with the difference
on how the AST is traversed. Each Clippy rule has special functions that are called on
various items in the language, like function and struct declarations. Each of these
special functions determine if there is anything relevant to report to the user for that
construct.

The linter hlint~\cite{hlint} is a linter for the compiled functional language
Haskell~\cite{haskell}. This linter specify rules in YAML, a typical format for
configuration, not in Haskell or a custom made language. This is possible since the rules
in hlint are relatively simple, they are pattern matches against the Haskell source code.
The suggested rewrite is also specified as plain Haskell code. One example rule finds
\texttt{not (a == b)} and suggest rewriting it to \texttt{a /= b}, the rewrite is shorter
and handles \texttt{NaN} correctly.

There are many terms for when a rule finds a match for something it wants to report on.
\emph{Hits}, \emph{matches}, \emph{finds}, \emph{gets triggered}, and \emph{reports} are
some that will be used in this report.

Rules in linters are usually divided into several groups so they can be disabled or
enabled more easily. An example of this could be to disable all rules about white space
and indentation. They also usually have unique identification numbers or names so they can
be individually disabled if desired.

\paragraph{Example}
The tool ShellCheck~\cite{shellcheck} is a linter for shell scripts like
bash~\cite{bash}. The construct \texttt{"\$@"} expands to all arguments passed to the
current script, useful if all arguments should be processed to determine the script's
behaviour. The quotes are vital, since the expression without them will perform additional
word splitting on the arguments (also something called ``globbing'', for completeness
sake), which is not desired in the majority of cases. Consider a script called with the
arguments ``-f'' and ``my cool model.mzn''. The expression \texttt{"\$@"} will expand in
to the same arguments, i.e., ``-f'' and ``my cool model.mzn''. But without quotes, i.e.,
\texttt{\$@}, it will instead expand to ``-f'', ``my'', ``cool'', and ``model.mzn'', because they
have been split on words. The lint
rule called ``SC2068'' in ShellCheck does exactly this analysis.

\subsection{Parsing and Abstract Syntax Trees}\label{sec:parsing}
To be able to process source code, or models in text format, it is common to parse them
into some kind of data structure. There are many tools available called ``parser
generators'', which are programs that takes some kind of formal description of the
language to be parsed and generates source code in some programming language for a parser.
MiniZinc uses a parser generator called Bison~\cite{flexbison}, which is a generator that
can generate C++ code, the same language that MiniZinc is written in.

The output of a parser is often an Abstract Syntax Tree (AST), an example of which is
given in Figure~\ref{fig:bkg:ast}. The AST is representing the formal structure of the
source code as nodes with sub-nodes, i.e., a tree. A function in C++ could
for example be represented as a node with a child for every statement in that function.
Each statement is itself a tree encoding whatever that statement represents. The
information about the arguments and the return type could be stored in the node itself or
as yet more children, there are many ways to design an AST.

\begin{figure}[ht]
  \centering
  \input{fig-bkg-ast}
  \caption{On the left is a possible tree of the expression $0\cdot(2+f(3))$. The right one
    is a possible tree of the expression $(0 \cdot 2)+f(3)$. The asterisks in the trees
    represents multiplication. The different placements of the parenthesis changes what the
    generated tree looks like. The red numbers on the sides represents what each sub-tree
    evaluates to if $f(x) = x$.}%
  \label{fig:bkg:ast}
\end{figure}

An AST is abstract in the sense that it does not encode everything in the original code.
Things like parentheses in mathematical expression are often implicitly encoded by the
position each operator has in the tree, an example of this is shown in
Figure~\ref{fig:bkg:ast}. Those trees are evaluated bottom up, and each node's value is
its operation applied to the results of its child nodes. Nodes high up in the tree are
therefore evaluated \emph{after} their child nodes, which means that sub-expressions that
should be evaluated first, like expressions in parentheses, are placed further down in the
tree.

\section{Implementation}\label{sec:impl}
This project is implemented using C++~\cite{cpp}, a compiled programming language with
performance as a main design goal. The same parser that the MiniZinc project is
using was reused in this project, both to save time and to simplify the implementation.
% This project will only analyse the Abstract Syntax
% Tree (AST) given from the parser, so it is unnecessary to write a new parser which would
% do the same thing.
The Abstract Syntax Tree (AST) will only be searched, no extensions or
modifications are thus needed, allowing the parser to be reused as is.
Using the same AST would also make it possible to reuse other utilities
present in the MiniZinc compiler, such as the type checker and pretty printer (converting
the AST back to nicely formatted code). Multiple complicated aspects of the language was
provided for free as a consequence of reusing the parser, and was thus not needed to be
reimplemented. One such aspect is function
resolution, i.e., the rules and logic behind what function definition to use given a name
and some arguments. Another reason
for reusing the parser is to make a potential integration into the original project easier,
as one parser is easier to maintain than two.

The MiniZinc
compiler is written in C++, creating this project in the
same language was thus a natural choice. C++ comes in several standards, and C++17 is the one
chosen for this project as that was the latest one all major compilers fully supported at
the time this project started. The MiniZinc version is 2.5.5 as, again, that was the
latest version when this project started.

An overview of how the program itself is structured is explained in
Section~\ref{sec:impl:structure}. The AST searcher will be explained in detail in
Section~\ref{sec:searcher}. How rules are added is explained in Section~\ref{sec:impl:rules}.

\subsection{Application Structure}\label{sec:impl:structure}
\begin{figure}[ht]
  \centering
  \smallskip%
  \input{fig-overview}
  \smallskip%
  \caption{Illustration for the main execution of the linter. The \cpp{LintEnv} is given as
    an argument to each \cpp{LintRule} (1). Each rule will analyse the AST and write its
    results back to a list inside \cpp{LintEnv} (2). When all rules have been processed, is
    the list of all results given to a function that will output them back to the user
    (3).}%
  \label{fig:overview}
\end{figure}

The overall structure of this linter is illustrated in Figure~\ref{fig:overview} and
everything following here will be illustrated in that figure. The most important component
of this linter is a base class called \cpp{LintRule}. Each rule
is its own class that inherits from \cpp{LintRule}. Each rule
consists of a unique ID, %a category,
a name, and a method that accepts a model that returns the rule's results, if any.
This model argument is an object of the class \cpp{Model}, i.e., the AST from the parser
in the MiniZinc project.
% They will be referred to as ``AST'' to avoid confusion with model files.
The ID and category are used for filtering which rules to use, and the name is used
for printing purposes. Each rule performs its analysis by first searching in the AST for a
relevant construct, which for example could be a variable used inside an
\mi{if}-statement, followed by multiple checks or other processing to figure out whether
this construct should be reported to the user or not. They do this search by using a
custom searcher described in Section~\ref{sec:searcher}.

\begin{sloppypar}
The rules do not take the AST directly, they take it wrapped in a class called
\cpp{LintEnv}. This class contains a lot of helper functions that each rule interacts with
directly. \cpp{LintEnv} contains, aside from the AST, a list of file paths
to directories that contain library files, this list is henceforth referred to as
``include paths''.
MiniZinc comes with a standard library that many
common MiniZinc functions are defined in (e.g.\@ global constraints), and include paths help the linter to find them.
\cpp{LintEnv} also contain all results and methods to create new ones. The final thing
this class does is caching the results from common searches performed by the AST searcher.
One such search is finding all user-defined
variables, both top-level and those defined in \mi{let}-statements.
\end{sloppypar}

Each result is recorded in a class called \cpp{LintResult}. These hold the relevant data
for a result like which rule it came from, the file position (line and column) for where it
matched, an explanatory message, a recommended rewrite of the model, etc. These results are
then given to some function that will present them to the user. That can be a
printed to standard output in a human-readable form, or converted to the JSON-format and
given to an IDE, or something else.

These are all of the relevant components for a high-level overview of this linter. A
typical execution for this linter in terms of these components is as follows:
\begin{enumerate}
  \item Gather options and model files from the arguments of the process
  \item Parse to file into a \cpp{Model} class (AST), abort if errors occurred
  \item Type check the AST, abort if errors occurred
  \item Construct a \cpp{LintEnv}-object
  \item Iterate through all \cpp{LintRule} and perform their analysis, saving their
  results to \cpp{LintEnv}.
  \item Give the list of all \cpp{LintResult} to a function that will output them to, e.g.\@,
  the terminal
\end{enumerate}

\subsection{Abstract Syntax Tree Searcher}\label{sec:searcher}
This project uses a custom made algorithm for finding relevant positions of interest in
MiniZinc Abstract Syntax Trees (ASTs). Many rules require more complex searches than
matching of single nodes in the AST, e.g., ``find x=3 anywhere inside constraints
except for in the else-clause of if-statements'' over ``find all if-statements''. Creating
a searcher for this made the implementation of all rules simpler. An alternative
approach would be to use what the MiniZinc project already is using for processing the
AST, namely visitors. Visitors would process the tree by running various functions on each
node, chosen by the type of the node~\cite[p.~331]{DesignPatterns94}. It would be easy to find
single nodes of interest using visitors, but it would be verbose to find
more complicated constructs.

\subsubsection{MiniZinc Abstract Syntax Tree}
\begin{figure}[ht]
  \centering
  \newif\ifshowastnumbers\showastnumberstrue
  \input{fig-ast-searcher}
  \caption{Illustration of two ASTs. The left one corresponds to \mi{constraint x+42=y},
    and the right one corresponds to \mi{var int: x=1+1+1+1}. The blue numbers are
    for referring to the nodes more easily and are not a part of the AST. The \mi{Id} at
    node 3 has a pointer to its declaration. \cpp{BinOp} is short for ``binary operation'',
    \cpp{Id} is short for ``identifier'', and lastly \cpp{IntLit} is short for ``integer
    literal''. These three are names of classes from MiniZinc.}%
  \label{fig:ast:searcher}
\end{figure}

The output of the MiniZinc parser is an object of the class \mi{Model}. It is not strictly
an abstract syntax \emph{tree}, but more like a collection of abstract syntax
\emph{graphs}. An illustration of this is shown in Figure~\ref{fig:ast:searcher}.
\mi{Model} contains a list of all top-level items in a model, like constraints, variable
declarations, and functions. Each of those contain one or more \emph{expressions}, which
basically are things that can be evaluated to some value, e.g., \mi{1+1} which can be
evaluated to \mi{2}. Each expression is not strictly a tree either since for example
\cpp{Id}-nodes, which represent an identifier like ``x'', has a pointer to the variable
declaration it is referring to. The searcher explained here will actually search in these
expressions individually, but the whole model will be referred to as one AST for
simplicity.

Each expression saves where it came from, they save: the filename, beginning line, end
line, beginning column, and end column. If an expression did not come directly from a file,
but was auto generated, then it is marked as ``introduced'' instead.

\subsubsection{Finding Paths}\label{sec:paths}
The searcher works by finding paths in the AST, i.e., a sequence of $k \in \elnaturale^+$
nodes $n_1, n_2, \dots ,n_k$ where $n_m$ is a direct child of $n_{m-1}$ for
all $m \in \{2,\dots,k\}$. A target path, or path to be searched for, is given as a
non-empty sequence of targets $T_{R,N}$. Where $N$ is what kind of AST node to match
against and $R \in \{U,D\}$ specifies how a node matched by a target relates to the node
matched by the previous target in the sequence. The value $D$ (``direct'') means that the
node matched should be a direct child of the previous match, and $U$ (``under'') means
that the node can be anywhere under the previous match. If $T_{R,N}$ is the first target
in the sequence, then $R$ is not referring to the previous match, but to an implicit dummy node
that only has the root as child.
A value of $D$ would therefore mean that the match has to be the root of the tree and $U$ would mean
that it can occur anywhere.

For example, the sequence $\langle T_{D,=} \; T_{U,+} \rangle$ will match a \cpp{BinOp}~=
at the root of the AST, and then match a \cpp{BinOp}~+ anywhere on either the left-hand
side or right-hand side of the equal-node. Taking the left tree in
Figure~\ref{fig:ast:searcher} as an example, this sequence will give the path
$\langle 1,2 \rangle$ as a match.

Another example is the target sequence
$\langle T_{U,+} \; T_{U,\text{\mi{IntLit}}} \rangle$, this will match a plus-node
anywhere that has an integer literal anywhere under it. The left tree in
Figure~\ref{fig:ast:searcher} has one path matching this, namely $\langle 2, 4 \rangle$.
The right tree on the same Figure has a couple of more matches, namely:
$\langle 6, 8 \rangle$, $\langle 6, 9 \rangle$, $\langle 6, 11 \rangle$,
$\langle 6, 12 \rangle$, $\langle 7, 8 \rangle$, $\langle 7, 9 \rangle$,
$\langle 10, 11 \rangle$, and $\langle 10, 12 \rangle$.

\subsubsection{Algorithm}\label{sec:algo}
The path matching described in Section~\ref{sec:paths} is implemented as a backtracking
algorithm. It will do a Depth First Search (DFS) to iterate over all paths in the AST. If
the algorithm at some points deduces that the current path can not possibly produce a full
match for a sequence of targets, then it will stop searching that path and backtrack to an
earlier point and try a different path.

The algorithm is given an
AST of $m \in \elnaturale^+$ nodes, and a sequence $S_1, S_2, \dots, S_k$ of $k \in \elnaturale^+$ targets as
explained in Section~\ref{sec:paths}. The algorithm maintains a stack $D$ for a DFS of the
AST, and another stack $P$ that contains the current path the DFS is on. Nodes get pushed
to $P$ as they are discovered, but the algorithm must also know when to pop them from $P$. For this,
the algorithm
has to know when a particular node's children are done searching. Let's say that the DFS
popped a new node $n$ from $D$, it will then push $n$ to $P$ to update the current path, but it will
also push it back to $D$, as a marker, followed by all children of $n$. Now, if at any point later in
the search $n$ gets popped again from $D$ and the top element of $P$ also is $n$, then the DFS
algorithm now knows that all sub-nodes of $n$ have been processed and that it is now okay to pop $P$.

Some nodes in $P$ will be matched nodes from $S_n$, and some will be ``nodes on the way''
when ``under'' is used. Only the matched nodes are of interest, so yet another stack $H$
will keep track of the same path as $P$ but only store nodes matched directly from all $S_n$.
$H$ is therefore a sub-stack of $P$, and
$H$ will have a maximum size of $k$ while $P$ will have a maximum size equal to
the height of the AST.

A function $U$
will determine whether a target $S_n$ is ``under'' (see Section~\ref{sec:paths}). $D$ is
initialised with the root node of the AST and $P = H = \emptyset$. $1 \leq x \leq k+1$ will be the index of
the next \emph{unmatched} target to find. If $x = k+1$, then are all targets matched.
% The pseudo code is shown in Figure~\ref{fig:algo:code}.
The pseudo code is as follows:

% \begin{figure}
\begin{enumerate}[noitemsep]
  \item\label{alg:beg} abort if $D = \emptyset$
  \item $d = \mathtt{pop}(D)$
  \item if $d = \mathtt{top}(P)$, a marker was popped
    \begin{enumerate}
      \item $\mathtt{pop}(P)$, $d$ is no longer on the current path
      \item if $d = \mathtt{top}(H)$
      \begin{enumerate}
        \item $\mathtt{pop}(H)$ and $x \gets x-1$
        \item if $U(S_x)$, then push $d$ to $P$ and $D$, and also all children of $d$ to $D$.
        This step forces $d$ to be unmatched to allow other nodes under $d$ to
        be matched against $S_x$.
      \end{enumerate}
      \item go to~\ref{alg:beg}
    \end{enumerate}
  \item if $S_x$ matches $d$
    \begin{description}
      \item[true] $x \gets x+1$, push $d$ to $H$ as it is a match
      \item[false] go back to \ref{alg:beg} if $U(S_x) = \top$. $S_x$ had to match
      directly after $S_{x-1}$ (the previous), but it did not
    \end{description}
  \item push $d$ to $P$, it is now part of the current path. Also push $d$ to $D$ as a marker
  \item if all $S$ have matched ($x > k$), then $P$ contains a whole match. Go to~\ref{alg:beg}
  \item push all children of $d$ to $D$
  \item go to~\ref{alg:beg}
\end{enumerate}
% \caption{}%
% \label{fig:algo:code}
% \end{figure}

\subsubsection{Run-time Analysis}\label{sec:algo:anal}
The worst case run-time complexity of the algorithm presented in Section~\ref{sec:algo}
mostly depend on how many ``under'' there are and what shape the
tree is in. The worst case is when all targets are ``under'', i.e., $\forall_{1 \leq x
  \leq k} : U(S_x)$. Consider that the AST is a straight line that is $m$
nodes long and that there are $k$ targets to match. If all $S_n$ match on every node, then the
searcher will iterate over all $\binom{m}{k}$ matches. An upper bound can be found from the
definition:
\begin{equation*}
  \binom{m}{k} = \frac{m!}{k! (m-k)!} = \frac{m}{k} \cdot \frac{m-1}{k-1} \dots \frac{m -
    (k-1)}{1} \leq \frac{m^k}{k!}
\end{equation*}%
The worst-case complexity is then $\Oh{{m^k}/{k!}}$, at least for when the AST is a line.
If the AST was instead a tree there would not be as many matches since not all
$\binom{m}{k}$ choices of nodes is on a path from the root, so it would not be as bad.
Discussions on this complexity is presented in Section~\ref{sec:discussion:searcher}.

\subsubsection{Filter Out the Standard Library}\label{sec:filter:stdlib}
Different kinds of filtering is performed by the searcher. Each target $T_{R,N}$
(explained in Section~\ref{sec:paths}) can have a function $F(n,c)$ that is used to
determine whether child $c$ of $n$ should be searched or not. This can be used to, for
example, make sure only the left-hand side of a binary operator is searched upon. In
general, it makes a search more specific and fine-tuned.

Only considering user-defined variables, functions, constraints etc.\@ is another kind of
filtering this searcher does. It is not of interest for the user to get lint results in
the standard library, the user can not modify those anyway (at least not if the model should
be usable by others with an unmodified standard library). Given an include path to the
standard library, the searcher will not search in functions and variables whose origin
filename is somewhere inside the include path. It will also ignore those whose location is
introduced, i.e., they have been generated by the compiler.

\subsubsection{C++ Interface}
\sloppy % section-wide sloppy

An instance of the \cpp{Search} class is constructed from a \cpp{SearchBuilder}. The
\cpp{Search} class contain all settings for a search operation, like the list of all
targets $T_{R,N}$ (see Section~\ref{sec:paths}). The
builder define many functions for specifying \emph{where} a search should take place and
\emph{what} to search for. All functions beginning with \cpp{in\_} specify what kind of
top-level items should be considered, no items are searched in by default. For example, calling
\cpp{in\_constraint} specify that all constraints should be searched in, and
\cpp{in\_everywhere} enable every item.
All targets $T_{R,N}$ are added via the functions \cpp{under} and \cpp{direct}, both of
which take the Id of a node to match against. To later
retrieve a pointer to a matched node, each target must explicitly be marked to
be saved, this is done via \cpp{capture}.
The final object is constructed via \cpp{build} when all customisation is done.

An example that search for addition-nodes (captured) inside equal-nodes is constructed as follows:
\begin{cppp}[style=nonumbers]
const Search s = SearchBuilder()
  .in_everywhere()
  .direct(BOT_EQ)
  .under(BOT_PLUS)
  .capture()
  .build();
\end{cppp}
The arguments are alternatives from an \cpp{enum} called \cpp{BinOpType} (Binary Operator Type).
It is also supported to specify values from \cpp{ExpressionId}, and \cpp{UnOpType} (Unary
Operator Type).

The value of \cpp{s} contain the information necessary for a search, but to actually
perform a search is yet another class constructed. Calling \cpp{s.search} will return a
\cpp{ModelSearcher} or \cpp{ExpressionSearcher}, depending on if the function was supplied
a pointer to a model or a pointer to an expression. Both types of return value implement
similar interfaces.
The rationale behind this design
decision is that \cpp{s} now can be used several times
without using the builder every time.
The difference between both concrete searchers is their granularity. \cpp{ModelSearcher}
searches on whole top-level items while \cpp{ExpressionSearcher} searches inside any kind
of expression at any level in the AST. The latter can be used to do further searches from
the results of the former.

The results are iterated one at a time in a lazy fashion by calling \cpp{next} on, e.g.,
\cpp{ModelSearcher}. The function returns true if there is a result available. Captured
targets can be retrieved by calling the function \cpp{capture}, supplied with a zero-based
index for which capture to return. The function \cpp{cur\_item} return a pointer to the
current top-level item that is currently being searched.

An example where a model is searched:
\begin{cppp}[style=nonumbers]
auto sear = s.search(model);
while(sear.next()) {
  const Expression *plus = sear.capture(0);
  const Item *item = sear.cur_item();
}
\end{cppp}

If no \cpp{under} or \cpp{direct} are called, then the built searcher is still valid and it will instead
iterate over all top-level items without searching for anything inside expressions.
The \cpp{SearchBuilder} can also construct \cpp{Search} objects that recursively visit
included files, which is not done by default, and also specify filters for each target as
explained in Section~\ref{sec:filter:stdlib}. The filters are added after an \cpp{under}
or \cpp{direct} by calling \cpp{filter} with a function pointer as argument (or lambda
without captures).

\fussy % reset section-wide sloppy
\subsection{Rule Implementation}\label{sec:impl:rules}
All rules have their own file in \texttt{src/linter/rules/}, and each one define a class
inheriting from \cpp{LintRule}. The one for \ruleref{nodomain} is defined in the following way:
\begin{cppp}[label=lst:impl:rule,caption={\null}]
namespace {
using namespace LZN;

class NoDomainVarDecl : public LintRule {
public:
  constexpr NoDomainVarDecl()
     : LintRule(13, "unbounded-variable") {}

private:
  virtual void do_run(LintEnv &env) const override {<@\dots@>}
};

} // namespace

REGISTER_RULE(NoDomainVarDecl)
\end{cppp}
The class is defined in an anonymous namespace, and the constructor for the parent class take
the necessary metadata for the rule, like its Id and name. The method \cpp{do\_run} do the
actual searching in the AST provided by its argument \cpp{env}. The macro at the very end
create a static instance of the class which a pointer of is given to a static registry.
In other words, the rule will be added to a global list of available rules before the main
function is executed.

The code for \cpp{do\_run} in the rule above (Listing~\ref{lst:impl:rule}) is currently defined as follows:
\begin{cppp}[label=lst:impl:dorun,caption={\null}]
for (const MiniZinc::VarDecl *vd
     : env.user_defined_variable_declarations()) {
  if (isNoDomainVar(*vd) && vd->e() == nullptr &&
      env.get_equal_constrained_rhs(vd) == nullptr)
  {
    auto &loc = vd->loc();
    env.emplace_result(
      FileContents::Type::OneLineMarked, loc, this,
      "no explicit domain on variable declaration"
    );
  }
}
\end{cppp}
All found variable declarations are looped over in the for-loop. Processing all variable
declarations is a common operation, so the method on line~2 is a wrapper around a searcher
that cache its results. The function call on line~3 returns true if the variable declaration is
a integer or float decision variable without an explicitly specified domain. It is a custom function
defined locally in the same file, it is defined as follows:
\begin{cppp}[style=nonumbers]
bool isNoDomainVar(const MiniZinc::VarDecl &vd) {
  auto &t = vd.type();
  auto domain = vd.ti()->domain();
  return t.isvar()
    && t.st() == MiniZinc::Type::SetType::ST_PLAIN
    && (t.bt() == MiniZinc::Type::BaseType::BT_INT ||
        t.bt() == MiniZinc::Type::BaseType::BT_FLOAT)
    && t.dim() >= 0
    && t.isPresent()
    && domain == nullptr;
}
\end{cppp}

If the variable declaration is assigned to another value, the MiniZinc compiler can deduce
the domain from that, in which case it is fine to omit the domain. The cached function on
line~4 in Listing~\ref{lst:impl:dorun} search for constraints that functionally define the
variable declaration, e.g., constraints like \mi{constraint x=2}.

\begin{sloppypar}
If a variable declaration meet all criteria, then the result is added to \cpp{env} via
\cpp{emplace\_result}. The first argument specify how the item at location \cpp{loc} should be
printed to the screen. Valid options are: printing nothing, printing several lines without
any highlighting, and printing a singe line with a portion of it highlighted with, e.g., a
squiggly line. In this case is the single highlighted line option used. The second
argument specify the the location, i.e., filename, start line, end line, start column, and end
column. The third argument is a pointer to the rule from which the result originates from.
Finally, the fourth argument is a string of a message explaining what the result is.
\end{sloppypar}

An excerpt for a relatively complicated searcher from the rule \ruleref{zeroone} is given
as follows:
\begin{cppp}
const auto main_searcher = env.userdef_only_builder()
  .in_everywhere()
  .under(BOT_IMPL)
  .capture()
  .filter([](const auto *impl, const auto *side) -> bool {
    return impl->template cast<MiniZinc::BinOp>()->lhs() == side;
  })
  .direct(BOT_EQ)
  .capture()
  .direct(E_INTLIT)
  .capture()
  .build();

const auto off_searcher = env.userdef_only_builder()
  .direct(BOT_EQ)
  .capture()
  .direct(E_INTLIT)
  .capture()
  .build();
\end{cppp}

The goal is to find expressions of the form \mi{a=1 -> b=1}.
The AST for it can be seen if redundant parentheses are added: \mi{((a)=(1)) -> ((b)=(1))}.
The implication is the root node, the left and right child are equal-nodes and both
equal-nodes have \mi{a}, \mi{b} or \mi{1} as children. Nodes \mi{a} and \mi{b} can in fact be
any kind of expression, as long as the structure of the AST is the same.

The searcher on line~1 search for an implication that has an equal-node as its left node,
and that equal-node has an integer literal as any direct child. The expression
\cpp{env.userdef\_only\_builder()} is a function that returns a \cpp{SearchBuilder} with some
modified default settings, e.g., recursively linting included non-library files is enabled.
When the main searcher has found a candidate is the right-hand side of the implication
searched with the off-searcher defined on line~14. If that searcher also succeeds, then the
overall search is complete if both integer literals are equal to \mi{1} and if \mi{a} and
\mi{b} have domains of $\{0,1\}$. The off-searcher can only find one result or none at
all, as it only uses \cpp{direct}. Annotated code for the combined search process is as follows:
\begin{cppp}
auto main = main_searcher.search(env.model());
while (main.next()) {
  //helper function to retrieve the other side
  //of a binary operation
  auto otherside = other_side(
    //another helper that retrieves a capture
    //and casts it to the correct type
    main.capture_cast<MiniZinc::BinOp>(0),
    main.capture(1)
    );
  auto off = off_searcher.search(otherside);
  if (!off.next())
    continue;
  <@\vdots@>
}
\end{cppp}

\section{Linting rules}\label{sec:rules}
In this section are all currently implemented rules explained, both why they are beneficial
and in some cases also how they are implemented.

The rules can point out constructs that should be avoided if possible. Some operations,
like dividing decision variables, is known to generally be bad in terms of solving time.
Other rules point out situations that can be rewritten in another way that is better in
some way. Producing fewer FlatZinc constraints is one example of an improvement. Fewer
constraints is a good indicator for good performance, but it is not a guarantee that fewer
constraints will result in better performance. Another similarly good indicator is the
number of decision variables.

The rules vary in complexity and preciseness. Some rules build graphs and provide
confident answers, while others simply mark decision variables in places where it is
\emph{generally} bad to have them. Some rules are more of a heads up than something that
should be fixed. Most rules are structured in a similar manner, they perform a search for
a relevant set of AST-nodes, followed by checks on those nodes to determine whether they
should be reported or not. One important detail is that the rules never modify the AST, they
are only searching and reading it.

One design decision was to not rely on instances, i.e., the rules should work even if all
parameters do not have values. The rationale is that all rules should be valid for all
instances of a given model. This limits the ability of some rules, notably
\ruleref{constvar}, as some conclusions it relies upon are impossible or difficult to prove.
Examples on conclusions and more limitations is discussion in Section~\ref{sec:disc:lintlimits}.

Further reading for more in-depth explanations on the implications of each rule is
available in the MiniZinc Handbook~\cite{mznbook}. Links to sections in that documentation
will be provided where relevant. The MiniZinc version and exact location can for the most
part be parsed from these links, useful in case they change in the future.

\subsection{Arrays Start at One}\label{sec:rule:arrayatone}
Arrays that start at 1 are more efficient and often easier to understand.
All user-defined arrays are examined and checked whether all their ranges is a set
literal, or a variable to a set literal, that starts at \mi{1}.
\begin{mznnobreak}
array[1..4] of var int: good;
array[5..9] of var int: bad;
\end{mznnobreak}
Flattened arrays always start at 1, so each array access to an array which do not start at
1 has to first be translated. If the access is from a decision variable, i.e., the \mi{i} in \mi{a[i]} is
\mi{var}, then an additional decision variable is introduced that translates \mi{i} to an index that
starts at one. More details can be found under ``Arrays'' in ``flattening'' in the
documentation.\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/flattening.html\#arrays}}

\subsection{Compactible If-statement}\label{sec:rule:compactif}
Some if-statements can be rewritten to a more compact form that tends to produce fewer
FlatZinc constraints. All if-statements are searched for and considered, and if they are on
the form:
\begin{mznnobreak}
var bool: b; var int: z; var int: y;
constraint z = if b then y else 0 endif;
constraint z = if b then 0 else y endif;
\end{mznnobreak}

can they be rewritten to more compact versions using implicit bool conversions (false converts
to 0 and true converts to 1):

\begin{mznnobreak}
constraint z = b*y;
constraint z = (not b)*y;
\end{mznnobreak}

\subsection{Constant Variable}\label{sec:rule:constvar}
A decision variable or an array of decision variables that are all assigned to non-variables should not be
marked with \mi{var} as their values are constant. It marks the intent of the variable more
clearly, and makes it easier for the compiler. Some examples where the keyword \mi{var}
should be omitted or replaced with \mi{par}:

\begin{mznnobreak}
var int: x = 2;
array[int] of var int: a = [1, 2, 3];
\end{mznnobreak}

This rule also checks if a \mi{var} is constrained to a \mi{par}-value:

\begin{mznnobreak}
var int: x;
constraint x = 2;
\end{mznnobreak}

All equality nodes with a mention to a variable as a direct child are considered, as long
as it is known that the equality always will hold. The same logic to determine if an
equality always hold is also used and explained in rule \ruleref{nonfuncdef}.

This rule also checks if all values in an array is constrained to \mi{par}-values for
simple cases, some limitations explained in Section~\ref{sec:disc:lintlimits}. The exact
formulation below (Listing~\ref{lst:constvar}) is searched for, i.e., a \mi{forall} that constrains every element to a
\mi{par}-value. The index sets has to be exactly the same, and there can not be a
where-clause on the comprehension.

\begin{mznnobreak}[label=lst:constvar,caption={\null}]
array[1..5] of var int: a;
constraint forall(i in 1..5)(a[i] = 1);
\end{mznnobreak}

\subsection{Effective 0..1 Variables}\label{sec:rule:zeroone}
Some expressions can be rewritten to a often better performing expression if the domains in
question happens to be $\{0,1\}$. For example, if there are two variables declared as
\mi{var 0..1: a} and \mi{var 0..1: b}, then can the expression \mi{a=1 -> b=1} be rewritten
to the equivalent \mi{a<=b}. In the same way can \mi{a=0 -> b=0} be rewritten to \mi{a>=b}.

The produced FlatZinc of the rewrites is shorter and simpler than the original. The
implication get reified, meaning that two additional Boolean decision variables get
introduced, one for each side. Additionally is three constraints produced, one for each
side and a third that constrains at least one of the previous to be true. A cleaned up
variant (no annotations and no predicate declarations) of the FlatZinc is as follows:
\begin{mznnobreak}
var 0..1: a;
var 0..1: b;
var bool: X_INTRODUCED_0_;
var bool: X_INTRODUCED_1_;
constraint array_bool_or([X_INTRODUCED_0_,X_INTRODUCED_1_],true);
constraint int_ne_imp(a,1,X_INTRODUCED_0_);
constraint int_eq_imp(b,1,X_INTRODUCED_1_);
\end{mznnobreak}
The rewrite on the other hand does not introduce any additional decision variables and
only a constraint for the inequality is produced.
\begin{mznnobreak}
array [1..2] of int: X_INTRODUCED_0_ = [1,-1];
var 0..1: a;
var 0..1: b;
constraint int_lin_le(X_INTRODUCED_0_,[a,b],0);
\end{mznnobreak}

\begin{sloppypar}
Additionally, this rule also suggests to rewrite \mi{sum(i in S)(a[i] = 1)} to the
equivalent \mi{sum(a)} if \mi{array[S] of var 0..1: a}. This one is especially good
as the first variant is doing implicit conversions between Booleans and integers
(\mi{bool2int}) while the rewrite is not.
\end{sloppypar}

The expressions on either side of the implications (\mi{a} and \mi{b}) can be arbitrarily
complex, as long as their domains can be calculated. If either depends on the current
instance, then a warning is issued, since if that parameter is changed, these rewrites
might not be valid any more, so the modeller should think twice before rewriting.

\subsection{Element Predicate}\label{sec:rule:element}
The predicate \mi{element} is a function that takes an index (\emph{i}), an array
(\emph{a}) and a value (\emph{v}) and returns true if the element at \emph{i} in the array
\emph{a} is equal to \emph{v}. \mi{element(i, a, v)} is a more verbose way of writing
\mi{a[i] = v}, \mi{element} is even defined as such. The array
access syntax should be used instead to make the model easier to read.

\subsection{Global Constraint Reified}\label{sec:rule:reifiedglobal}
A global constraint called in a reified context \emph{can} be slow and inefficient,
and this rule will mark all those occasions. Being reified means that the result of the global
constraint is bound to a Boolean variable, i.e.,
\begin{mznnobreak}
var bool: x = all_different(a);
\end{mznnobreak}
Reification happens when the global constraint is called in a context that is not $\land$.
For example, this global constraint will be reified:
\begin{mznnobreak}
constraint b \/ all_different(a);
\end{mznnobreak}
and the following is always okay:
\begin{mznnobreak}
constraint all_different(a);
constraint all_different(b) /\ all_different(c);
\end{mznnobreak}
The official MiniZinc documentation talks more about this under ``Reified and half-reified
predicates''\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/fzn-spec.html\#reified-and-half-reified-predicates}}
and under ``Reification''.\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/flattening.html\#reification}}

The linter will consider all functions from the standard library as global constraints,
except for those that do not have to be explicitly included, like \mi{forall}.

\subsection{Global Variables in Functions}\label{sec:rule:globalfun}
Using variable from the global scope in functions is usually confusing as the whole model
(or at least everything required to understand the purpose of the global variable in
question) has to be read and understood to understand the function. It is also not
immediately as clear as to what variables a function is constraining and accessing. This rule
suggest taking all those variables as arguments to the function instead.

\begin{mznnobreak}
var int: g;
function int: f() = g+1;
constraint f() = 2;
\end{mznnobreak}
is suggested to be rewritten to:
\begin{mznnobreak}
var int: g;
function int: f(var int: x) = x+1;
constraint f(g) = 2;
\end{mznnobreak}

Only decision variables are marked as parameter values can only be read from, and are like
parameters to the whole model itself, so they have to be understood anyway.

\subsection{No Domain on Variables}\label{sec:rule:nodomain}
Variables can specify a domain in which the variable can take values from. It is always
recommended to specify a tight domain for decision variables as not specifying one at all
can make the domain \emph{very} large and makes the solving unnecessarily slow.
\begin{mznnobreak}
var int: bad;
var 0..5: good;
\end{mznnobreak}
It is fine to omit the domain if a variable is assigned to a value, even another variable,
as the domain can be referred from the assigned value, constraining them is also fine.
\begin{mznnobreak}
var 10..20: good;
var int: also_good = good;
var int: also_good2;
constraint also_good2 = good;
\end{mznnobreak}
This rule marks unassigned decision variables that have no explicit domain given.
More can be read in ``Bounds analysis''.\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/efficient.html\#variable-bounds}}

\subsection{Non-functionally Defined Variables Not in Search Annotation}\label{sec:rule:nonfuncdef}
One of the rules of the MiniZinc challenge~\cite{MZN:Challenge} states: ``Each solve item
must be annotated with a search strategy, such that fixing all the variables appearing in
the search strategy would allow a value propagation solver to check a solution''. A rough
approximation of this is to require all non-functionally defined variables to be included
in the search annotation. This rule marks all such variables that are \emph{not} in the
search annotation. This rule is thus only valuable in the context of the MiniZinc
challenge.

A decision variable is functionally defined if it is uniquely defined in terms of other
variables. An example of this is a variable constrained under equality to some expression.
In the example below (Listing~\ref{lst:nonfuncdef}) is the variable \mi{a} functionally defined in terms of \mi{b} and \mi{c}.
If \mi{b} and \mi{c} are fixed to some values, then can the value of \mi{a} be obtained. In this
case must both \mi{b} and \mi{c} be included in the search annotation, it is optional for
\mi{a} to be included.
Assignment is also a way to functionally defined variables, i.e., \mi{var int: a=5*b+c}.
\begin{mznnobreak}[label=lst:nonfuncdef,caption={\null}]
var int: a; var int: b; var int c;
constraint a = 5*b + c;
solve
  :: int_search([b, c], input_order, indomain)
  satisfy;
\end{mznnobreak}

Functionally defined variables are searched for by finding \cpp{BinOp}-nodes of type
``equality'' with an \cpp{Id}-node to a variable as direct child. Top-level constraints
are searched and the equality can by arbitrarily deep in those, as long as the equality is
guaranteed to hold. The equality hold if it is under conjunction nodes (\mi{/\\},
\mi{forall}), under debugging functions (\mi{trace}) etc. An example where \mi{a} and
\mi{b} are both functionally defined: \mi{a=b+1 /\\ trace("I'm here!", c=a*2)}.

Arrays are approximated in the sense that they are treated as one unit. This means that
only one value in the array needs to be functionally defined for the whole array to be
considered functionally defined, i.e., \mi{xs[1]=3}. This limitation is present because
it is difficult to know whether all values in an array are functionally defined or not
(see Section~\ref{sec:disc:lintlimits}). Since all values in arrays usually are
constrained together with \mi{forall}, this approximation is not too incorrect.

Another way a decision variable can be functionally defined is via global constraints (or
any predicate). For example, \mi{count(zs, 2, z)} constrains \mi{z} to be equal to the
number of times the value \mi{2} occurs in the array \mi{zs}, which means that \mi{z} is
functionally defined. A possible definition of \mi{count} is:
\begin{mznnobreak}[label=lst:count,caption={\null}]
predicate count(array[int] of var int: xs,
                var int: y, var int: c) =
  c = sum(i in xs)(i = y);
\end{mznnobreak}

The function definitions of all function calls inside top-level constraints, that are
guaranteed to hold, are searched. If a parameter of the function definition is
functionally defined, then the corresponding argument is also functionally defined. In the
example above (Listing~\ref{lst:count}) is \mi{c} functionally defined in the definition
of \mi{count}, that means that \mi{z} in the function call \mi{count(zs, 2, z)} also is
functionally defined. The definitions are recursively searched for as long as the calls
are in a conjunctive context (\mi{/\\} etc.).

There are some limitations unique to these predicate searches:
\begin{itemize}
  \item The expression \mi{a=b} functionally defines both variables in terms of each other.
  At least one of those variables need to be present in the search annotation so the other
  can be calculated. But since both are considered functionally defined are none reported
  by this rule.
  \item
  \begin{sloppypar}
  Functions that return variables are currently not checked for. For example, \mi{id(x)=2}, where
  \mi{id} is the identity function (returning the argument as is) will not consider \mi{x}
  to be functionally defined, even though it is.
  \end{sloppypar}
  \item The arguments to the function calls can only be the names of top-level variables,
  the actual variables can not be deduced otherwise, except for in simple cases. For
  example, \mi{f(xs)} is fine since the argument can be deduced to the top-level decision
  variable \mi{xs}. The Expression \mi{f(g(xs, ys))} is not fine as the origin of the
  values passed to \mi{f} can not be deduced. A special case is the function \mi{array1d},
  which converts an n-dimensional array to a one-dimensional array, since it is common in
  the standard library.
  \item Functions that constrain top-level variables via constraints in
  \mi{let}-statements are currently not searched for.
\end{itemize}

\subsection{Operators on Variables}\label{sec:rule:opvar}
Some operators like \mi{div} and \mi{pow} are expensive to do on variables. Pre-computing
these and storing them in a table (tabling) is recommended, if feasible. The expensive
calculations are done once, before the solving has started, instead of multiple times
during solving.

Operators like \mi{\\/} and \mi{->} on variables can also be expensive as they introduce
more branching and have worse propagation in solvers, at least on Constraint Logic
Programming solvers~\cite[p.~430]{constraintshandbook}.

This rule blindly recommends to avoid using these operators on variables.

\subsection{Symmetry Breaking Missing}\label{sec:rule:symbreak}
Some global constraints like \mi{value_precede_chain} are almost exclusively used to break
symmetries in models. Breaking symmetries is important since it speeds up solving by
reducing the amount of possible solutions that has to be explored. Constraints whose
purpose is break symmetries should be marked as such:
\begin{mznnobreak}
constraint symmetry_breaking_constraint(<@\dots@>);
\end{mznnobreak}
Some solving techniques, like local search, are negatively impacted
by symmetry breakers, so solvers that use technologies like that ignore all marked
symmetry breaking constraints. So this rule blindly marks global constraints that are
normally used for breaking symmetries. More general theory about this can be found in
``Effective modelling practices''.\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/efficient.html\#symmetry}}

\subsection{Unused Variables and Functions}\label{sec:rule:unused}
This rule report on variables and functions that are not used anywhere.
Used in this case means to be mentioned inside constraints, the solve statement or other
functions and variables that are also in use. The output-statement is an exception and
do not contribute to the usage of variables and functions. The rationale is that only being
mentioned in the output statement does not contribute to the model, i.e., the amount of
solutions do not change.

To calculate whether a variable or function is unused is a dependency graph first
constructed. This directed graph will keep track of which variables and functions use
which other variables and functions. The graph for the following example is shown in
Figure~\ref{fig:unused:graph}.

\begin{mznnobreak}
int: K; int: N;
int: M = let {int: J = 5} in J+N;
var 0..K: x;
function var int: f() = x+N;
solve minimize f();
\end{mznnobreak}

\begin{figure}[ht]
  \centering
  \smallskip%
  \input{fig-unused-graph}
  \smallskip%
  \caption{A dependency graph of an example model. An edge from a node to another means
    that the first node is directly depending on the other. For example, \mi{M} is using
    \mi{N}. The green filled in nodes (\mi{f}, \mi{N}, \mi{x}, and \mi{K}) are in use, and
    the blue filled node (\mi{J}) is ignored as it is contained inside of \mi{M}.}%
  \label{fig:unused:graph}
\end{figure}

After the graph has been constructed are all constraints and solve-statements checked for
mentions of any variables and functions. Each one and all of its
dependants are marked as used. In this example is \mi{f} used in the solve-statement, so
\mi{f}, \mi{N}, \mi{x}, and \mi{K} are thus marked as used. In this case, there are no
constraints, so \mi{J} and \mi{M} are left as unused.

Reporting both \mi{J} and \mi{M} in this case might seem excessive since \mi{J} is
``inside'' of \mi{M}, if \mi{M} is unused, then it follows that \mi{J} also is unused.
This is even more of a problem with unused functions, as each argument and variable inside
of it also will be individually reported. To solve this and only report on the outermost
variable or function is another graph constructed, a containment graph. In this case is
only \mi{J} inside another variable or function, namely \mi{M}. This containment graph is
inspected after all variables and functions have been deemed used or unused, and each
unused variable or function that is inside something else, that also is unused, gets
ignored. So in this case is only \mi{M} reported to the user as unused.

\subsection{Variables in Generators}\label{sec:rule:vargen}
This rule blindly reports on decision variables used in generators. This is generally
bad and should be avoided.

\begin{mznnobreak}
var 1..5: x;
constraint forall(i in 1..x)(<@\dots@>);
\end{mznnobreak}

A \mi{forall} is unrolled into several constraints when flattened, one for each
value of \mi{i} in this case. But when the exact amount of constraints is unknown, the
flattener must do more complicated things. More about unrolling can be read in ``Unrolling
Expressions''\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/flattening.html\#unrolling-expressions}}
and ``Hidden Option Types''.\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/optiontypes.html\#hidden-option-types}}

\subsection{Variables in If and Where}\label{sec:rule:varif}
This rule will blindly mark decision variables used in \mi{where} clauses and the deciding
part on \mi{if}-statements, as that can slow down solving. Examples of this are:

\begin{mznnobreak}
var bool: b;
constraint if b then <@\dots@> else <@\dots@> endif;
\end{mznnobreak}
\begin{mznnobreak}
array[<@\dots@>] of var int: a;
constraint forall(i in <@\dots@> where a[i] > 5)(<@\dots@>);
\end{mznnobreak}

These will make the models more complex and should preferably be avoided if possible.
More about the \mi{where} case can be read about under ``Hidden Option
Types''.%\footnote{\url{https://www.minizinc.org/doc-2.5.5/en/optiontypes.html\#hidden-option-types}}.

\section{In-depth Review of a Model}\label{sec:nsp}
A model from the MiniZinc Benchmarks~\cite{mznbench} will be linted, modified according to
the linter's suggestions and benchmarked. The model studied is ``Nurse Scheduling
Problem'' (NSP), written by Nina Narodytska in 2007-12-01.
The unmodified files are provided in Appendix~\ref{app:nsp}.

\subsection{Model Description}
The NSP problem is a satisfaction problem where \mi{n_nurses} are to be scheduled over a
period of \mi{sched_period} days. Each day has \mi{n_shifts} shifts, and each nurse may be
assigned to a single shift each day. There is a coverage requirement, i.e., each shift
require a minimum amount of nurses to be assigned to each shift. The required coverage is
a number between 0 and \mi{n_nurses}. The required coverage is given in the parameter
\mi{nurses_coverage}, which is a 2D-array where \mi{nurses_coverage[p,s]} is the number of
required nurses for day \mi{p} on shift \mi{s}. An example instance, derived sets used for
arrays and parameters about regulation rules (explained later) is shown in
Table~\ref{tab:nsp:inst}. The exact declaration for the required coverage is:
\begin{mznnobreak}
array[period, shifts] of 0..n_nurses: nurses_coverage;
\end{mznnobreak}

\begin{table}[ht]
\begin{minipage}[t][][b]{0.40\textwidth}
\centering
\begin{tabular}{p{2cm}rrr}
  \multicolumn{4}{c}{\mi{nurses_coverage}}\\\hline
  Day & \multicolumn{3}{c}{Shifts} \\\hline
  % & \multicolumn{3}{c}{Shifts} \\\hline
  % Day & 1 & 2 & 3\\\hline
  1& 4 & 3 & 1 \\
  2& 0 & 0 & 0 \\
  3& 0 & 2 & 1 \\
  4& 2 & 1 & 1 \\
  5& 4 & 4 & 2 \\
  6& 1 & 2 & 0 \\
  7& 1 & 1 & 0 \\
  8& 2 & 2 & 5 \\
  9& 4 & 5 & 2 \\
  10& 3 & 1 & 3 \\
  11& 2 & 0 & 1 \\
  12& 2 & 1 & 1 \\
  13& 2 & 2 & 1 \\
  14& 1 & 2 & 0
\end{tabular}
\end{minipage}%
\begin{minipage}[t][][b]{0.55\textwidth}
\centering
\begin{tabular}{lr}
  Parameter & Value \\\hline
  \mi{n_nurses} & 16 \\
  \mi{sched_period} & 14 \\
  \mi{n_shifts} & 3 \\
  \mi{n_rules} & 2 \\\hline
  \mi{period} & \mi{1..sched_period} \\
  \mi{shifts} & \mi{1..n_shifts} \\
  \mi{shifts_and_off} & \mi{1..n_shifts+1} \\
  \mi{nurses} & \mi{1..n_nurses} \\
  \mi{rules} & \mi{1..n_rules}\\\hline
  \mi{rules_sets} & \mi{[\{4\},\{3\}]} \\
  \mi{rules_lbs} & \mi{[1,0]} \\
  \mi{rules_ubs} & \mi{[2,1]} \\
  \mi{rules_windows} & \mi{[3,3]}
\end{tabular}
\end{minipage}
\caption{The values of instance \texttt{nsp/period\_14/1.dzn}.}%
\label{tab:nsp:inst}
\end{table}

The model's decision variables are:
\begin{mznnobreak}[label=lst:vars,caption={\null}]
array[nurses, period] of var shifts_and_off: nurses_schedule;
array[period, shifts] of var int: coverage;
\end{mznnobreak}
The first array, \mi{nurses_schedule}, specify for each nurse, on each day, what shift the
nurse should work on. A special value of \mi{n_shifts+1} is used to indicate that the
nurse does not work on any shift that day. The second array, \mi{coverage}, specify how
many nurses are scheduled to work on each shift each day.

The first constraint constrains the decision variable \mi{coverage} to meet the minimum
demand in \mi{nurses_coverage}.
\begin{mznnobreak}[caption={\null}]
constraint forall (i in period, j in shifts) (
   coverage[i,j] >= nurses_coverage[i,j]
);
\end{mznnobreak}

The two decision variable arrays are connected, if a nurse \mi{n} on day \mi{p} works on
shift \mi{s=nurses_schedule[n,p]}, then should \mi{coverage[p,s]} reflect that. More precisely,
\mi{coverage[p,s]} should be equal to the number of nurses who are scheduled to work on
shift \mi{s} on day \mi{p}. The occurrences of all type of shifts should be counted on all nurses on
day \mi{p}, and to help accomplish that is an array of all shifts \mi{1..n_shifts}
constructed. It is done as follows:
\begin{mznnobreak}[label=lst:shifts,caption={\null}]
array [shifts] of var int: shifts_values;
constraint forall (j in shifts) (
  shifts_values[j] = j
);
\end{mznnobreak}
\mi{shifts_values} is constrained to \mi{[1,2,3]} in this case.

A predicate \mi{day_distribute} is defined to set the necessary constraints to connect the
two decision variables for a single day. Each day is then constrained by this predicate
with a \mi{forall}:
\begin{mznnobreak}[caption={\null}]
constraint forall (i in period) (
  day_distribute(i)
);
\end{mznnobreak}
The predicate is defined as follows:
\begin{mznnobreak}[label=lst:distribute,caption={\null}]
predicate day_distribute(int: i) = let {
  array [shifts] of var int: row_coverage =
    [coverage[i,j]| j in shifts]
  }
  in distribute (
    row_coverage,
    shifts_values,
    [nurses_schedule[j,i]| j in nurses]
    );
\end{mznnobreak}
\begin{sloppypar}
  \mi{row_coverage} is the current coverage of all shifts for day \mi{i}. The global
  constraint \mi{distribute} counts how many times all values in the middle argument occur
  in the third array and stores those counts in the first array. For example,
  \mi{row_coverage[1]} is equal to the number of times \mi{shifts_values[1]} occur in
  \mbox{\mzninlinebar{[nurses_schedule[j,i] | j in nurses]}}, which is an array with the
  shifts all nurses are scheduled to work on for the current day, and so on for all other
  shifts.
\end{sloppypar}

There is one more aspect to this model, there are \mi{n_rules} regulation rules. Each rule
specify the upper and lower bound for how many times a nurse can be scheduled to a
set of shifts over a fixed time window. For example, in Table~\ref{tab:nsp:inst} are two rules
specified, the first one constrains each nurse to have one or two days off (shift 4) over
all groups of three consecutive days. These rules are added to the model in the same
manner as the previous one, i.e., a predicate and a \mi{forall}.
\begin{mznnobreak}[caption={\null}]
constraint forall (i in rules, j in nurses) (
  apply_rule_for_nurse(i, j)
);
\end{mznnobreak}
The following predicate makes sure regulation rule \mi{i} hold for nurse \mi{j}:
\begin{mznnobreak}[label=lst:rules,caption={\null}]
predicate apply_rule_for_nurse (int: i, int: j) = let {
  array [period] of var 0..1: rule_for_nurse
  } in
  forall (k in period) (
    (nurses_schedule[j,k] in rules_sets[i])
    <->
    rule_for_nurse[k] = 1
  ) /\
  sliding_sum (
    rules_lbs[i],
    rules_ubs[i],
    rules_windows[i],
    rule_for_nurse
  );
\end{mznnobreak}
This predicate define an array \mi{rule_for_nurse}, if \mi{rule_for_nurse[r]=1}, then is nurse
\mi{j} scheduled to a ``target'' (\mi{rules_sets}) of rule \mi{i}. The \mi{forall} specify
this relationship for all days. \mi{sliding_sum} is a global constraint that constrains
all sliding sums of a specified length in an array to be between an upper and lower bound.
For example, \mi{sliding_sum(0, 2, 2, [1, 2, 3, 4])} is the same as $0 \leq 1+2 \leq 2 \land
0 \leq 2+3 \leq 2 \land 0 \leq 3+4 \leq 2$.

\subsection{Linter Results}\label{sec:nsp:results}
The model was rewritten to satisfy the suggestions from the linter and each rewrite got
benchmarked.
The solvers I had access to were COIN\babelhyphen{nobreak}BC~\cite{coinbc} (version
2.10.5/1.17.5), Gecode~\cite{gecode} (version 6.3.0) and Chuffed~\cite{Chuffed} (version 0.10.4).
No concrete numbers are presented since there were no meaningful results to show.
COIN\babelhyphen{nobreak}BC performed the same no matter the edit, only a few milliseconds
of difference between runs, which could be measurement noise. Gecode did not find
solutions in a short amount of time (under 10~minutes), so nothing can be said about this
solver. Chuffed varied drastically in its solve times, which is expected since it is
sensitive to its input, so no conclusions could be drawn from this solver either.
Most suggested edits did however make the model simpler and hopefully easier to understand.

Each modification to the model will be displayed in a format similar to \texttt{diff -u},
i.e., red lines prefixed with \texttt{-} indicate a line from the \emph{old} model that
got removed, and green lines prefixed with \texttt{+} indicate \emph{new} lines that got added.

\paragraph{Suggestion 1}
\begin{sloppypar}
The first suggestion is on Listing~\ref{lst:shifts}, where rule \ruleref{constvar} noted
that \mi{shifts_values} is only constrained to constant values. That is true,
\mi{shifts_values} will always be constrained to the array \mi{[1,2,3]} (for the example instance).
There is no need for this to be a decision variable, the array can be rewritten as a parameter in the following way:
\end{sloppypar}
\begin{mznnobreak}[style=diff]
- array [shifts] of var int: shifts_values;
- constraint forall (j in shifts) (
-   shifts_values[j] = j
- );
+ array [shifts] of int: shifts_values =
+   [j | j in shifts];
\end{mznnobreak}
This edit makes it more clear as to what \mi{shift\_values} is, especially since the
constraint and variable declaration are separated in the original model file.

\paragraph{Suggestion 2}
The second rule to match was \ruleref{nodomain} on the decision variable \mi{coverage} in
Listing~\ref{lst:vars}, this array does not have a domain on its variables. Since there is
a fixed number of nurses, can not a single shift ever have more nurses than the maximum, there also
can not be a negative amount of nurses assigned to a shift. \mi{coverage} has in fact the
same domain as \mi{nurses_coverage}.
\begin{mznnobreak}[style=diff]
array[nurses, period] of var shifts_and_off: nurses_schedule;
- array[period, shifts] of var int: coverage;
+ array[period, shifts] of var 0..n_nurses: coverage;
\end{mznnobreak}

\paragraph{Suggestion 3}
The third rule to match was \ruleref{opvar} on the implication in Listing~\ref{lst:rules}.
The implication can be removed by utilising implicit \mi{bool2int} conversion on the
left-hand side of the implication, as follows:
\begin{mznnobreak}[style=diff]
predicate apply_rule_for_nurse (int: i, int: j) = let {
- array [period] of var 0..1: rule_for_nurse
+ array [period] of var 0..1: rule_for_nurse =
+   [(nurses_schedule[j,k] in rules_sets[i])
+    | k in period];
  } in
- forall (k in period) (
-   (nurses_schedule[j,k] in rules_sets[i])
-   <->
-   rule_for_nurse[k] = 1
- ) /\
  sliding_sum (
    rules_lbs[i],
    rules_ubs[i],
    rules_windows[i],
    rule_for_nurse
  );
\end{mznnobreak}

\paragraph{Suggestion 4}
Rule \ruleref{globalfun} had matches in both predicates for using the decision
variables without taking them as arguments to the predicates. Nothing was changed here as
it is unclear whether an improvement in readability would have been made, the predicates
are used to make their respective \mi{forall} smaller. There also should not be any
performance change at all since all constraints, in the end, are the same.

\paragraph{Additional Edit}
\begin{sloppypar}
This is not a suggestion from the linter itself, but an
observation from applying the previous suggestions.
The global constraint \mi{distribute} in Listing~\ref{lst:distribute} can take all of its
arguments as decision variables. There is a similar global constraint called
\mi{global_cardinality} that does the same thing, except that the middle argument can
only accept parameters, and that the first and third argument swapped places. Since
\mi{shifts_values} does not consist of decision variables anymore can
\mi{global_cardinality} be used instead.
\end{sloppypar}
\begin{mznnobreak}[style=diff]
predicate day_distribute(int: i) = let {
  array [shifts] of var int: row_coverage =
    [coverage[i,j]| j in shifts]
  }
- in distribute (
-   row_coverage,
+ in global_cardinality (
+   [nurses_schedule[j,i]| j in nurses],
    shifts_values,
-   [nurses_schedule[j,i]| j in nurses]
+   row_coverage
    );
\end{mznnobreak}

\section{Discussion}\label{sec:discussion}
This section will feature discussions on running the linter on all models in the MiniZinc
Benchmarks~\cite{mznbench}, on the limits on what this linter can do, and on my experience
of using the source code of MiniZinc.

\subsection{Searcher Performance}\label{sec:discussion:searcher}
It took in total around 23 seconds to lint all 299\footnote{Was actually 300, but model
  ``mznc2009\_roster\_model.mzn'' contained annotations that do not exist anymore, so it was
  removed.} model
files in the MiniZinc Benchmarks~\cite{mznbench}. It took in total around 13 seconds when
only running the parser and type checker. That means that around $(23-13)/299 = 0.033$
seconds (\SI{33}{\milli\second}) was spent on checking all rules on average, the tests
were performed on my average desktop computer.

The exponential
worst-case complexity presented in Section~\ref{sec:algo:anal} does not seem to cause too much
of an issue. Most of all top-level expressions in all models in the MiniZinc Benchmarks are
relatively small, i.e., they have a few number on AST nodes,
so the input was not too big to cause an issue.
A few top-level expression had a very large number
of nodes though, but they also seemed like they were auto generated.
The size of the AST of all top-level items are presented in Figure~\ref{fig:ast:counts}.

\input{fig-ast-counts}

The exponential behaviour is even less of a problem, since in the final implementation was
only \emph{one} instance of the searcher using two ``under'' targets (the cause of the
exponential behaviour), all other had one or none at all. The worst-case analysis also
assumes that each target matched on all nodes in the AST, but that is not a common
occurrence as each target matches a \emph{single} type of node, and there are many
different kinds. The searcher will therefore most often than not stop early in the tree,
so most of all $\binom{m}{k}$ cases are not even checked.

\subsection{Searcher Limitations}
The searcher presented in Section~\ref{sec:searcher} searches for a path in the AST, and
that has been sufficient for most rules, but the rule \ruleref{zeroone} required a little
bit more. That rule had to find expression on the form \mi{a=1 -> b=1}, but the current
searcher can only look for \mi{a=1 -> rhs}, i.e., only one of the sides. This problem was
circumvented by starting another search on \textit{rhs} that looked for \mi{b=1} after a
position for the left-hand side and implication was found. This particular example is
explained in Section~\ref{sec:impl:rules}. A future work could be to implement this kind
of functionality on the searcher itself, i.e., adding the ability to search for
\emph{sub-trees}.

Several polynomial-time algorithms for searching, or matching, sub-trees are presented in
a thesis by Kilpeläinen~\cite{kilpelainen92}. % I didn't bother to setup natbib or biblatex
The algorithms differ in how the \emph{patterns} looks like, i.e., what is searched for.
There is no single algorithm in that thesis that covers all use cases of the AST searcher,
but some of these algorithms could selectively be used depending on how the pattern to
search for looks like. Simple patterns can use a simpler, and therefore faster, algorithm.
The two most interesting algorithms to use in that thesis are ``unordered tree inclusion''
and ``unordered path inclusion''. Compared to the current AST searcher, the first matches
trees where everything is ``under'', and the second matches where everything is
``direct''. The first is proven in the thesis to be NP-complete while the second has a
polynomial-time algorithm. Creating a new searcher that chooses between these two
algorithms is not enough since the current AST searcher offers patterns with a mix of
both ``under'' and ``direct''. The work in that thesis can however, if anything, be used as a
starting point for a new AST searcher that supports searching for sub-trees.

\subsection{Reflection on Matches in the MiniZinc Benchmarks}
There were many rule hits throughout all models in the MiniZinc Benchmarks, which is not
surprising as some rules are general recommendations by design, so most of these hits are
probably false positives. It does not mean that they are useless though, as they will make
the modeller think twice about, e.g., dividing a decision variable with another. But it
might also get annoying to see many yellow squiggly lines in an IDE or text editor. These
general suggestions can probably be fine-tuned to specific solvers in the future, reducing
the number of false positives.

All rules were triggered at least twice when run on all models in the MiniZinc benchmarks,
except for \ruleref{vargen}, which was not hit even once. This indicates that it
either is an obscure feature, or that people know that it is usually slow.

\ruleref{opvar} and \ruleref{reifiedglobal} had many hits,
which is expected since those rules are broad and do not do many exotic checks.
Doing implications and the like on variables is also common, so these rules are maybe too broad.

It was surprising to see that there were any models at all that had no domains on their
decision variables, and models that assigned constant values to their decision variables.
There maybe are domain deductions the MiniZinc compiler can perform that I am not aware
of, which other modellers are using.

The final surprising thing was that there were a lot of matches for \ruleref{unused},
it was in fact the \emph{most} matched one. I did not look at all of them, but
there were models that had variables that were only declared and never mentioned again,
and those should definitely be removed.
Some looked like they were forgotten model parameters from previous iterations of the model.
I think that this
rule will be very useful to have. It is a lot easier to understand a model if it only contains
what it needs to contain, less to read and less to understand.

There were some unused variables that were only mentioned in the output-statement. The
output statement should maybe count towards usage after all? It seemed common to output
some calculated statistics from a solution, like the maximum element in an array. There is
an \mi{output_only} annotation that could maybe be used here to allow variables only used
in output to exist. Someone more knowledgeable will have to figure out the answer to that
question.

\subsection{Linter Limitations}\label{sec:disc:lintlimits}
Sometimes it is not possible to circumvent a rule without explicitly ignoring it. For
example, to produce an array filled with zeros can the expression \mzninlinebar{[0 | i in 1..k]} be
used. But \mi{i} will be reported here as being unused, and there is nothing to do about
it. It is a parser error to write \mzninlinebar{[0 | \_ in 1..k]} instead. So the lint rule must be
ignored here, or a special case should be introduced for that rule.

It is also possible to obfuscate a model to make a rule not match at all. Consider
the example in \ruleref{zeroone}, where
\mi{a=1 -> b=1} could be rewritten to \mi{a<=b} since both variables have a domain of
$\{0,1\}$. It is equally valid to write \mi{a>=1 -> b=1}, or \mi{(true /\\ a=1) -> b=1},
but the linter will not recognise those cases. It is unreasonable to write special cases for
all of these since there are so many, and most people will not write \mi{true /\\ a=1} anyway.

The independence of specific instances introduces some problems, or rather cases where
some conclusion can not be proved.
An example of this is whether a \mi{forall}
accesses all values in an array or not. The following model demonstrates a case
where it is impossible to know:

\begin{mznnobreak}
int: N; int: K;
array[1..N] of var int: a;
constraint forall(i in 1..K)(a[i] = <@\dots@>);
\end{mznnobreak}

The \mi{forall} accesses all values if \mi{N} is equal to \mi{K}, but that is not
guaranteed to be the case, so the linter will assume that the \mi{forall} does not access
all values. If the \mi{forall} used the same set as the array (\mi{1..N}), then the
linter would know that all values were accessed. Some rules will provide results anyway,
even if they depend on parameters, but the linter will warn the user in those cases so an
accidental rewrite that breaks the model in the future is not made.

The author of the original lint~\cite{lint} talk about similar problems with lint, i.e.,
that some facts are impossible to prove. For example, determining whether a function is
unused can depend on runtime data, a function is maybe only called if a certain file
exists, and that is impossible to determine statically, at least in general. So original
lint compromises and only assumes a function is unused if it is never
mentioned~\cite{lint}. This linter makes a similar compromise where the rule
\ruleref{unused} assumes a function is used if it is mentioned in a constraint. The actual
call could be in a context where it does not matter what the return value of the call is,
but the linter will assume that it is used anyway.

\subsection{Using the MiniZinc Parser}
Not having to create a correct and conforming parser and type checker that followed the
MiniZinc language specification made the process of implementing this project easier.
There were some quirks that had to be worked around though, as the parser and type
checker, ideally, should not be modified. There were for example \mi{enum}-declarations
from the standard library that looked like they were declared in the main file being
linted, and not the file they were originally defined in, so those had to be specially
filtered away.

Some generator expressions had the ordering on their \mi{where} clauses swapped around for
optimisation purposes \emph{before} flattening. That also made that expression introduced,
so the original file position information where it came from got lost. That causes
problems for the standard out printer, which needs to know where that generator is written
so it can print it exactly as is.

The method for figuring out whether something is user-defined or whether it comes from the
standard library was not the prettiest. As explained in Section~\ref{sec:filter:stdlib},
they are filtered if the file they are defined in comes from a include path. It is not
surprising that a good way of checking that does not exist. The MiniZinc compiler do not
really need to know exactly where a function came from, it only needs a definition.

Integer literals are handled specially in the AST, presumably to save on memory. The
literals are stored in nodes of type \mi{IntLit}, but they are interned, meaning that
nodes with the same contained integer value are shared. For example, all integer literals
of value $1$ in different locations in the AST refer to the same object in memory. There
is a global list that keeps track of all currently allocated \mi{IntLit} created. There is
also a mechanism that stores the integers \emph{in} the pointers themselves, if they fit,
avoiding an indirection. The problem with both of these approaches is that integer
literals do not save the location of where they originated from. This means that the
standard out printer fails and prints an error if it tries to print a sole integer
literal. A way to circumvent this would be to use the pretty printer when printing a
location that is flagged as introduced.

A similar annoyance is that the locations of binary operations, e.g., \mi{/\\}, are not
saved either. The locations of the two sides are available though, so the location of the
operator itself can be approximated by taking the location between the left and the right
side.

\section{Future work}
At the moment, the linter executable can only accept a single model-file as an argument,
the include path can not be modified and the linter will always output the results with
colours to standard out.
Some options and configuration should be added to make the linter more useful. Notably
the ability to choose what rules to use and what to ignore. Being able to choose what
rules to use as command line arguments and to ignore individual matches by adding special
comments in the model itself are useful to have. In other words, a proper command-line
interface needs to be implemented.

The rules presented in this report are far from everything that is interesting to check
for. More rules should be added over time to facilitate future needs, maybe even solver
specific ones for better accuracy. A relatively simple way to create new rules without
recompiling the whole program should get more thought. Specifying rules in configuration
files in a similar manner to ESLint and hlint would be a good way forward. Contribution of
new rules to the base linter would also get easier if this more accessible rule creation
method is implemented. A flexible and expressive rule specification language would have to
be designed to allow for as many kinds of rules as possible to be expressed in it.
Extending the AST searcher to allow searching for sub-trees, not only paths, would most
likely be needed to achieve a good specification language. Eventually could all rules,
even the builtin ones, be converted to that format.

Many of the ways a model can be obscured in could be counteracted by performing a
simplification pass of the AST, before any rules are run. This pass could for example find
expressions of the form: \mi{false \\/ a} and convert them to the equivalent \mi{a}, where
\mi{a} is any valid expression. This pass could also find \mi{[a]} and replace with \mi{a}
if it is type-safe and the semantics is preserved. A specific example is that the rule
\ruleref{nonfuncdef} can not deduce that \mi{a} is functionally defined in the expression
\mi{[a] = [b+1]}. Solutions could be to introduce a special case in the rule itself, or to
run this simplification pass to rewrite the expression as \mi{a = b+1}, which the rule
would find. A simplification pass would allow the rules to become simpler as each one
would not need to handle as many special cases.

%%%% Referenser - SE OCKÅ APPENDIX

% Use one of these:
%   IEEEtranS gives numbered references like [42] sorted by author,
%   IEEEtranSA gives ``alpha''-style references like [Lam81] (also sorted by author)
%\bibliographystyle{IEEEtranS}
\bibliographystyle{IEEEtranSA}

% Here comes the bibliography/references.
% För att göra inställningar för IEEEtranS/SA kan man använda ett speciellt bibtex-entry @IEEEtranBSTCTL,
% se IEEEtran/bibtex/IEEEtran_bst_HOWTO.pdf, avsnitt VII, eller sista biten av IEEEtran/bibtex/IEEEexample.bib.
\newpage
\bibliography{bibconfig,astra-bib/astra,refs}
% \printbibliography

\newpage
\appendix

\iffast\else
\section{Nurse Scheduling Problem Sources}\label{app:nsp}
The raw files of the model described in Section~\ref{sec:nsp} are displayed here. The only
modification is on lines 1 and 25 in \texttt{nsp\_1.mzn}, those dividers were shortened to
fit the page. The directory where all files are located is
\texttt{minizinc-benchmarks/nsp/} in the Git repository~\cite{mznbench} on commit
\texttt{26bcd0a78433025f7b6896a6fa8c\-af128795760b}.
\subsection{nsp\_1.mzn}
\lstinputlisting[language=Mzn,style=mzn]{nsp_src/nsp_1.mzn}
\subsection{test.rules}
\lstinputlisting[language=Mzn,style=mzn]{nsp_src/test.rules}
\subsection{period\_14/1.dzn}
\lstinputlisting[language=Mzn,style=mzn]{nsp_src/1.dzn}
\fi

\end{document}
